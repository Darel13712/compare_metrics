{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy from repository (commit 14e765b on 26 Aug 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/c173c9688c12e8b2866ac5f707e555faf811996f/Base/Evaluation/Evaluator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "@author: Maurizio Ferrari Dacrema, Massimo Quadrana\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import unittest\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "class _Metrics_Object(object):\n",
    "    \"\"\"\n",
    "    Abstract class that should be used as superclass of all metrics requiring an object, therefore a state, to be computed\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{:.4f}\".format(self.get_metric_value())\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "###############                 ACCURACY METRICS\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "class MAP(_Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Average Precision, defined as the mean of the AveragePrecision over all users\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MAP, self).__init__()\n",
    "        self.cumulative_AP = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant, pos_items):\n",
    "        self.cumulative_AP += average_precision(is_relevant, pos_items)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_AP/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MAP: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_AP += other_metric_object.cumulative_AP\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "\n",
    "\n",
    "def average_precision(is_relevant, pos_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        a_p = 0.0\n",
    "    else:\n",
    "        p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "        a_p = np.sum(p_at_k) / np.min([pos_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    assert 0 <= a_p <= 1, a_p\n",
    "    return a_p\n",
    "\n",
    "\n",
    "class MRR(_Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank, defined as the mean of the Reciprocal Rank over all users\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MRR, self).__init__()\n",
    "        self.cumulative_RR = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant):\n",
    "        self.cumulative_RR += rr(is_relevant)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_RR/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MRR: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_RR += other_metric_object.cumulative_RR\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "\n",
    "def roc_auc(is_relevant):\n",
    "\n",
    "    ranks = np.arange(len(is_relevant))\n",
    "    pos_ranks = ranks[is_relevant]\n",
    "    neg_ranks = ranks[~is_relevant]\n",
    "    auc_score = 0.0\n",
    "\n",
    "    if len(neg_ranks) == 0:\n",
    "        return 1.0\n",
    "\n",
    "    if len(pos_ranks) > 0:\n",
    "        for pos_pred in pos_ranks:\n",
    "            auc_score += np.sum(pos_pred < neg_ranks, dtype=np.float32)\n",
    "        auc_score /= (pos_ranks.shape[0] * neg_ranks.shape[0])\n",
    "\n",
    "    assert 0 <= auc_score <= 1, auc_score\n",
    "    return auc_score\n",
    "\n",
    "\n",
    "\n",
    "def arhr(is_relevant):\n",
    "    # average reciprocal hit-rank (ARHR) of all relevant items\n",
    "    # As opposed to MRR, ARHR takes into account all relevant items and not just the first\n",
    "    # pag 17\n",
    "    # http://glaros.dtc.umn.edu/gkhome/fetch/papers/itemrsTOIS04.pdf\n",
    "    # https://emunix.emich.edu/~sverdlik/COSC562/ItemBasedTopTen.pdf\n",
    "\n",
    "    p_reciprocal = 1/np.arange(1,len(is_relevant)+1, 1.0, dtype=np.float64)\n",
    "    arhr_score = is_relevant.dot(p_reciprocal)\n",
    "\n",
    "    assert not np.isnan(arhr_score), \"ARHR is NaN\"\n",
    "    return arhr_score\n",
    "\n",
    "\n",
    "def precision(is_relevant):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def precision_recall_min_denominator(is_relevant, n_test_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / min(n_test_items, len(is_relevant))\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "\n",
    "def recall(is_relevant, pos_items):\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / pos_items.shape[0]\n",
    "\n",
    "    assert 0 <= recall_score <= 1, recall_score\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def rr(is_relevant):\n",
    "    # reciprocal rank of the FIRST relevant item in the ranked list (0 if none)\n",
    "\n",
    "    ranks = np.arange(1, len(is_relevant) + 1)[is_relevant]\n",
    "\n",
    "    if len(ranks) > 0:\n",
    "        return 1. / ranks[0]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ndcg(ranked_list, pos_items, relevance=None, at=None):\n",
    "\n",
    "    if relevance is None:\n",
    "        relevance = np.ones_like(pos_items)\n",
    "    assert len(relevance) == pos_items.shape[0]\n",
    "\n",
    "    # Create a dictionary associating item_id to its relevance\n",
    "    # it2rel[item] -> relevance[item]\n",
    "    it2rel = {it: r for it, r in zip(pos_items, relevance)}\n",
    "\n",
    "    # Creates array of length \"at\" with the relevance associated to the item in that position\n",
    "    rank_scores = np.asarray([it2rel.get(it, 0.0) for it in ranked_list[:at]], dtype=np.float32)\n",
    "\n",
    "    # IDCG has all relevances to 1, up to the number of items in the test set\n",
    "    ideal_dcg = dcg(np.sort(relevance)[::-1])\n",
    "\n",
    "    # DCG uses the relevance of the recommended items\n",
    "    rank_dcg = dcg(rank_scores)\n",
    "    \n",
    "    if rank_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    ndcg_ = rank_dcg / ideal_dcg\n",
    "\n",
    "    return ndcg_\n",
    "\n",
    "\n",
    "def dcg(scores):\n",
    "    return np.sum(np.divide(np.power(2, scores) - 1, np.log(np.arange(scores.shape[0], dtype=np.float32) + 2)),\n",
    "                  dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    21307932\n",
       "4.5       25887\n",
       "5.0       12924\n",
       "Name: relevance, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.read_csv('./preds_full.csv')\n",
    "df_test = pd.read_csv('./test.csv')\n",
    "\n",
    "df_pred_new = pd.merge(\n",
    "    df_pred, \n",
    "    df_test.loc[:, ['user_id', 'item_id', 'relevance']], on=['user_id', 'item_id'], how='left'\n",
    ")\n",
    "\n",
    "df_pred_new = df_pred_new[df_pred_new.user_id.isin(df_test.user_id)]\n",
    "\n",
    "df_pred_new.fillna(0, inplace=True)\n",
    "\n",
    "df_pred_new.relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use code from repository\n",
    "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/Evaluator.py#L307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "roc_auc_list = []\n",
    "recall_list = []\n",
    "hitrate_list = []\n",
    "ndcg_list = []\n",
    "mrr_obj = MRR()\n",
    "map_obj = MAP()\n",
    "\n",
    "\n",
    "for user in sorted(df_pred_new.user_id.unique()):\n",
    "    is_relevant_current_cutoff = df_pred_new[df_pred_new.user_id == user].relevance.astype(bool).values[0:cutoff]\n",
    "    recommended_items_current_cutoff = df_pred_new[df_pred_new.user_id == user].item_id.astype(int).values[0:cutoff]\n",
    "    relevant_items = df_test[df_test.user_id == user].item_id.values\n",
    "    relevant_items_score = df_test[df_test.user_id == user].relevance.values#.astype(int).values\n",
    "    \n",
    "    precision_list.append(precision(is_relevant_current_cutoff))\n",
    "    roc_auc_list.append(roc_auc(is_relevant_current_cutoff))\n",
    "    recall_list.append(recall(is_relevant_current_cutoff, relevant_items))\n",
    "    hitrate_list.append(is_relevant_current_cutoff.sum())\n",
    "\n",
    "    ndcg_list.append(\n",
    "        ndcg(\n",
    "            recommended_items_current_cutoff, relevant_items, \n",
    "            relevance=relevant_items_score, at=cutoff\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mrr_obj.add_recommendations(is_relevant_current_cutoff)\n",
    "    map_obj.add_recommendations(is_relevant_current_cutoff, relevant_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "averaging or call get_metric_value:\n",
    "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/Evaluator.py#L255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at[\"Precision@20\",\"value\"] = np.mean(precision_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at[\"Recall@20\",\"value\"] = np.mean(recall_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at[\"AUC@20\",\"value\"] = np.mean(roc_auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at[\"HitRate@20\",\"value\"] = np.mean(hitrate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at[\"MRR@20\",\"value\"] = mrr_obj.get_metric_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at[\"MAP@20\",\"value\"] = map_obj.get_metric_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at[\"NDCG@20\",\"value\"] = np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
