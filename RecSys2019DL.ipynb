{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/c173c9688c12e8b2866ac5f707e555faf811996f/Base/Evaluation/Evaluator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "@author: Maurizio Ferrari Dacrema, Massimo Quadrana\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import unittest\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "class _Metrics_Object(object):\n",
    "    \"\"\"\n",
    "    Abstract class that should be used as superclass of all metrics requiring an object, therefore a state, to be computed\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{:.4f}\".format(self.get_metric_value())\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "###############                 ACCURACY METRICS\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "class MAP(_Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Average Precision, defined as the mean of the AveragePrecision over all users\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MAP, self).__init__()\n",
    "        self.cumulative_AP = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant, pos_items):\n",
    "        self.cumulative_AP += average_precision(is_relevant, pos_items)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_AP/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MAP: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_AP += other_metric_object.cumulative_AP\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "\n",
    "\n",
    "def average_precision(is_relevant, pos_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        a_p = 0.0\n",
    "    else:\n",
    "        p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "        a_p = np.sum(p_at_k) / np.min([pos_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    assert 0 <= a_p <= 1, a_p\n",
    "    return a_p\n",
    "\n",
    "\n",
    "class MRR(_Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank, defined as the mean of the Reciprocal Rank over all users\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MRR, self).__init__()\n",
    "        self.cumulative_RR = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant):\n",
    "        self.cumulative_RR += rr(is_relevant)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_RR/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MRR: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_RR += other_metric_object.cumulative_RR\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "\n",
    "def roc_auc(is_relevant):\n",
    "\n",
    "    ranks = np.arange(len(is_relevant))\n",
    "    pos_ranks = ranks[is_relevant]\n",
    "    neg_ranks = ranks[~is_relevant]\n",
    "    auc_score = 0.0\n",
    "\n",
    "    if len(neg_ranks) == 0:\n",
    "        return 1.0\n",
    "\n",
    "    if len(pos_ranks) > 0:\n",
    "        for pos_pred in pos_ranks:\n",
    "            auc_score += np.sum(pos_pred < neg_ranks, dtype=np.float32)\n",
    "        auc_score /= (pos_ranks.shape[0] * neg_ranks.shape[0])\n",
    "\n",
    "    assert 0 <= auc_score <= 1, auc_score\n",
    "    return auc_score\n",
    "\n",
    "\n",
    "\n",
    "def arhr(is_relevant):\n",
    "    # average reciprocal hit-rank (ARHR) of all relevant items\n",
    "    # As opposed to MRR, ARHR takes into account all relevant items and not just the first\n",
    "    # pag 17\n",
    "    # http://glaros.dtc.umn.edu/gkhome/fetch/papers/itemrsTOIS04.pdf\n",
    "    # https://emunix.emich.edu/~sverdlik/COSC562/ItemBasedTopTen.pdf\n",
    "\n",
    "    p_reciprocal = 1/np.arange(1,len(is_relevant)+1, 1.0, dtype=np.float64)\n",
    "    arhr_score = is_relevant.dot(p_reciprocal)\n",
    "\n",
    "    assert not np.isnan(arhr_score), \"ARHR is NaN\"\n",
    "    return arhr_score\n",
    "\n",
    "\n",
    "def precision(is_relevant):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def precision_recall_min_denominator(is_relevant, n_test_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / min(n_test_items, len(is_relevant))\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "\n",
    "def recall(is_relevant, pos_items):\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / pos_items.shape[0]\n",
    "\n",
    "    assert 0 <= recall_score <= 1, recall_score\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def rr(is_relevant):\n",
    "    # reciprocal rank of the FIRST relevant item in the ranked list (0 if none)\n",
    "\n",
    "    ranks = np.arange(1, len(is_relevant) + 1)[is_relevant]\n",
    "\n",
    "    if len(ranks) > 0:\n",
    "        return 1. / ranks[0]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ndcg(ranked_list, pos_items, relevance=None, at=None):\n",
    "\n",
    "    if relevance is None:\n",
    "        relevance = np.ones_like(pos_items)\n",
    "    assert len(relevance) == pos_items.shape[0]\n",
    "\n",
    "    # Create a dictionary associating item_id to its relevance\n",
    "    # it2rel[item] -> relevance[item]\n",
    "    it2rel = {it: r for it, r in zip(pos_items, relevance)}\n",
    "\n",
    "    # Creates array of length \"at\" with the relevance associated to the item in that position\n",
    "    rank_scores = np.asarray([it2rel.get(it, 0.0) for it in ranked_list[:at]], dtype=np.float32)\n",
    "\n",
    "    # IDCG has all relevances to 1, up to the number of items in the test set\n",
    "    ideal_dcg = dcg(np.sort(relevance)[::-1])\n",
    "\n",
    "    # DCG uses the relevance of the recommended items\n",
    "    rank_dcg = dcg(rank_scores)\n",
    "    \n",
    "    if rank_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    ndcg_ = rank_dcg / ideal_dcg\n",
    "\n",
    "    return ndcg_\n",
    "\n",
    "\n",
    "def dcg(scores):\n",
    "    return np.sum(np.divide(np.power(2, scores) - 1, np.log(np.arange(scores.shape[0], dtype=np.float32) + 2)),\n",
    "                  dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    51516\n",
       "1.0     3144\n",
       "Name: relevance, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pred = pd.read_csv('./pred.csv')\n",
    "\n",
    "df_test = pd.read_csv('./test.csv')\n",
    "\n",
    "df_pred_new = pd.merge(\n",
    "    df_pred, \n",
    "    df_test.loc[:, ['user_id', 'item_id', 'relevance']], on=['user_id', 'item_id'], how='left'\n",
    ")\n",
    "\n",
    "df_pred_new = df_pred_new[df_pred_new.user_id.isin(df_test.user_id)]\n",
    "\n",
    "df_pred_new.fillna(0, inplace=True)\n",
    "\n",
    "df_pred_new.loc[df_pred_new.relevance != 0, 'relevance'] = 1\n",
    "\n",
    "df_pred_new.relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1256</td>\n",
       "      <td>5378</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007-10-31 12:18:24</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1256</td>\n",
       "      <td>778</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2007-10-31 12:19:51</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1256</td>\n",
       "      <td>8376</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007-10-31 12:30:59</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1256</td>\n",
       "      <td>2594</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007-10-31 12:20:49</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1256</td>\n",
       "      <td>30810</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2007-10-31 12:32:24</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating            timestamp  relevance\n",
       "0     1256     5378     5.0  2007-10-31 12:18:24        5.0\n",
       "1     1256      778     4.5  2007-10-31 12:19:51        4.5\n",
       "2     1256     8376     5.0  2007-10-31 12:30:59        5.0\n",
       "3     1256     2594     5.0  2007-10-31 12:20:49        5.0\n",
       "4     1256    30810     4.5  2007-10-31 12:32:24        4.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['relevance_bin'] = (df_test.relevance > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1256</td>\n",
       "      <td>5378</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007-10-31 12:18:24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1256</td>\n",
       "      <td>778</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2007-10-31 12:19:51</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1256</td>\n",
       "      <td>8376</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007-10-31 12:30:59</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1256</td>\n",
       "      <td>2594</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007-10-31 12:20:49</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1256</td>\n",
       "      <td>30810</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2007-10-31 12:32:24</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating            timestamp  relevance  relevance_bin\n",
       "0     1256     5378     5.0  2007-10-31 12:18:24        5.0              1\n",
       "1     1256      778     4.5  2007-10-31 12:19:51        4.5              1\n",
       "2     1256     8376     5.0  2007-10-31 12:30:59        5.0              1\n",
       "3     1256     2594     5.0  2007-10-31 12:20:49        5.0              1\n",
       "4     1256    30810     4.5  2007-10-31 12:32:24        4.5              1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "посмотрел как работает HoldOut evaluator и взял код от туда:\n",
    "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/Evaluator.py#L307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "roc_auc_list = []\n",
    "recall_list = []\n",
    "hitrate_list = []\n",
    "ndcg_list = []\n",
    "mrr_obj = MRR()\n",
    "map_obj = MAP()\n",
    "arhr_list = []\n",
    "\n",
    "\n",
    "for user in sorted(df_pred_new.user_id.unique()):\n",
    "    is_relevant_current_cutoff = df_pred_new[df_pred_new.user_id == user].relevance.astype(bool).values[0:cutoff]\n",
    "    recommended_items_current_cutoff = df_pred_new[df_pred_new.user_id == user].item_id.astype(int).values[0:cutoff]\n",
    "    relevant_items = df_test[df_test.user_id == user].item_id.values\n",
    "    relevant_items_score = df_test[df_test.user_id == user].relevance_bin.astype(int).values\n",
    "    \n",
    "    precision_list.append(precision(is_relevant_current_cutoff))\n",
    "    roc_auc_list.append(roc_auc(is_relevant_current_cutoff))\n",
    "    recall_list.append(recall(is_relevant_current_cutoff, relevant_items))\n",
    "    hitrate_list.append(is_relevant_current_cutoff.sum())\n",
    "\n",
    "    ndcg_list.append(\n",
    "        ndcg(\n",
    "            recommended_items_current_cutoff, relevant_items, \n",
    "            relevance=relevant_items_score, at=cutoff\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mrr_obj.add_recommendations(is_relevant_current_cutoff)\n",
    "    map_obj.add_recommendations(is_relevant_current_cutoff, relevant_items)\n",
    "\n",
    "    arhr_list.append(arhr(is_relevant_current_cutoff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде метрики просто усредняют или вызывают get_metric_value:\n",
    "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/Evaluator.py#L255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057519209659714604"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09632131280310642"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(recall_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2748816881385198"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(arhr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28317447310115185"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(roc_auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.150384193194292"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(hitrate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47457006952067327"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0 if x <=0 else 1 for x in hitrate_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18632413351027025"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr_obj.get_metric_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03927176078866242"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_obj.get_metric_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07981995794175147"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-rinchin",
   "language": "python",
   "name": "dev-rinchin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
